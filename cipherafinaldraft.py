# -*- coding: utf-8 -*-
"""Cipherafinaldraft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYZHLz_LkwdHd3gpyTlB6G_YrlIHPHkU
"""

!pip install git+https://github.com/openai/whisper.git
import whisper
!pip install ultralytics
!pip install kagglehub
!pip install nudenet
!pip install transformers

from google.colab import drive
drive.mount('/content/drive')

"""##Importing Libraries"""

import cv2
from ultralytics import YOLO
import nudenet
from nudenet import NudeDetector
import os
from moviepy.editor import VideoFileClip
from transformers import pipeline
import whisper
from tqdm import tqdm

"""##Foul Language Detection"""

file_path = '/content/drive/MyDrive/abuse.mp4'
print('File Exists', os.path.exists(file_path))

video_path = '/content/drive/MyDrive/abuse.mp4'
audio_output = "extracted_audio.wav"

print("🎬 Extracting audio from video...")

video = VideoFileClip(video_path)
video.audio.write_audiofile(audio_output)

print("🗣️ Transcribing audio...")

model=whisper.load_model("large")
result = model.transcribe(audio_output, language = "en")
transcription = result["text"]

print("Transcription: ", transcription)

if transcription:
    print("🤖 Detecting foul language using BERT...")

    classifier = pipeline("text-classification", model="unitary/toxic-bert")
    results = classifier(transcription)

    foul_labels = ['toxic', 'obscene', 'insult', 'threat']
    foul_detected = False

    for res in results:
        label = res['label'].lower()
        score = res['score']
        print(f"🔍 {label} → Score: {score:.2f}")
        if label in foul_labels and score > 0.1:
            foul_detected = True

    if foul_detected:
        print("🔞 Foul language detected in the video!")
    else:
        print("✅ No foul language detected.")
else:
    print("⚠️ Skipping BERT classification due to transcription issue.")

"""##NSFW Image Detection"""

from nudenet import NudeDetector
Detect = NudeDetector()
result = Detect.detect("/content/drive/MyDrive/man.jpg")

print(result)

def is_nudity_detected(detections, threshold=0.5):
    nudity_classes = {
        'FEMALE_BREAST_EXPOSED',
        'MALE_BREAST_EXPOSED',
        'FEMALE_GENITALIA_EXPOSED',
        'MALE_GENITALIA_EXPOSED',
        'BUTTOCKS_EXPOSED',
        'UNDERWEAR_EXPOSED',
        'BELLY_EXPOSED'
    }

    strong_evidence = 0

    for det in detections:
        cls = det['class']
        score = det['score']
        if cls in nudity_classes and score >= threshold:
            strong_evidence += 1
    return strong_evidence >= 2

result1 = Detect.detect("/content/drive/MyDrive/download.jpg")

if is_nudity_detected(result1):
    print("Nudity Detected")
else:
    print("Safe Content")

"""##NSFW Video Detection

"""

detector = NudeDetector()

video_path = "/content/drive/MyDrive/abuse.mp4"
output_folder = "/content/drive/MyDrive/Colab\ Notebooks/"
frame_interval = 30  # process every 30th frame
nsfw_threshold = 0.7  # customize threshold for what you consider NSFW

file_path='/content/drive/MyDrive/abuse.mp4'
print('File Exists', os.path.exists(file_path))

os.makedirs(output_folder, exist_ok=True)

def extract_and_classify_frames(video_path, interval=30):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_rate = cap.get(cv2.CAP_PROP_FPS)
    nsfw_frames = []

    print(f"Processing video: {video_path}")
    print(f"Total Frames: {total_frames}, FPS: {frame_rate}")

    frame_num = 0
    with tqdm(total=total_frames) as pbar:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_num % interval == 0:
                frame_path = os.path.join(output_folder, f"frame_{frame_num}.jpg")
                cv2.imwrite(frame_path, frame)

                result = detector.detect(frame_path)
                if result:
                    label = result[0]['label']
                    score = result[0]['unsafe']

                    if label == 'unsafe' and score >= nsfw_threshold:
                        nsfw_frames.append((frame_num, score))

            frame_num += 1
            pbar.update(1)

    cap.release()
    return nsfw_frames

nsfw_detected = extract_and_classify_frames(video_path, frame_interval)
if nsfw_detected:
    print("\nNSFW content detected in the following frames:")
    for frame_num, score in nsfw_detected:
        print(f"Frame {frame_num} - Score: {score:.2f}")
else:
    print("\nNo NSFW content detected.")